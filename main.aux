\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{lillicrap2016random}
\citation{pdp,elman,medler}
\citation{ilker1}
\citation{yamins2}
\citation{rumelhart:86}
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\@newglossary{acronym}{alg}{acr}{acn}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{hebb1,paulsen}
\citation{oja,sejnowski1,sejnowski2}
\citation{lillicrap2016random}
\citation{lillicrap2016random}
\citation{nokland2016direct}
\citation{launay2020direct}
\citation{frenkel2021learning}
\citation{akrout,bellec,lillicrap2020backpropagation}
\citation{bartunov}
\citation{lillicrap2016random}
\@writefile{toc}{\contentsline {section}{\numberline {2}Problem Statement and Overview of Results}{2}{section.2}}
\newlabel{eqn:nonlinear-network}{{2.1}{2}{Problem Statement and Overview of Results}{equation.2.1}{}}
\newlabel{eqn:nonlinear-network@cref}{{[equation][1][2]2.1}{[1][2][]2}}
\newlabel{eqn:squared-loss}{{2.2}{2}{Problem Statement and Overview of Results}{equation.2.2}{}}
\newlabel{eqn:squared-loss@cref}{{[equation][2][2]2.2}{[1][2][]2}}
\citation{lillicrap2016random}
\citation{lillicrap2016random,bartunov,lillicrap2020backpropagation}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Feedback Alignment\relax }}{3}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{algo:fa}{{1}{3}{Feedback Alignment\relax }{figure.caption.1}{}}
\newlabel{algo:fa@cref}{{[algorithm][1][]1}{[1][3][]3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Standard backpropagation updates the first layer weights for a hidden node $r$ with the second layer feedforward weight $\beta _r$. We study the procedure where the error is backpropagated instead using a fixed, random weight $b_r$.\relax }}{3}{figure.caption.1}}
\newlabel{fig:algo}{{1}{3}{Standard backpropagation updates the first layer weights for a hidden node $r$ with the second layer feedforward weight $\beta _r$. We study the procedure where the error is backpropagated instead using a fixed, random weight $b_r$.\relax }{figure.caption.1}{}}
\newlabel{fig:algo@cref}{{[figure][1][]1}{[1][3][]3}}
\newlabel{eqn:alignment-update}{{2.3}{3}{Problem Statement and Overview of Results}{equation.2.3}{}}
\newlabel{eqn:alignment-update@cref}{{[equation][3][2]2.3}{[1][3][]3}}
\citation{lillicrap2016random}
\citation{lillicrap2016random}
\citation{lillicrap2020backpropagation}
\citation{jacot2018neural,chen2020deep}
\citation{jacot2018neural}
\citation{du2018gradient,du2019gradient,gao2020model}
\@writefile{toc}{\contentsline {section}{\numberline {3}Convergence with Random Backpropagation Weights}{4}{section.3}}
\citation{lillicrap2016random}
\newlabel{assump:G}{{3.1}{5}{}{theorem.3.1}{}}
\newlabel{assump:G@cref}{{[assumption][1][3]3.1}{[1][5][]5}}
\newlabel{thm:nonliner_conv}{{3.2}{5}{}{theorem.3.2}{}}
\newlabel{thm:nonliner_conv@cref}{{[theorem][2][3]3.2}{[1][5][]5}}
\newlabel{eq:conv}{{3.1}{5}{}{equation.3.1}{}}
\newlabel{eq:conv@cref}{{[equation][1][3]3.1}{[1][5][]5}}
\newlabel{eq:weights}{{3.2}{5}{}{equation.3.2}{}}
\newlabel{eq:weights@cref}{{[equation][2][3]3.2}{[1][5][]5}}
\newlabel{prop:positive-definiteness}{{3.3}{5}{}{theorem.3.3}{}}
\newlabel{prop:positive-definiteness@cref}{{[proposition][3][3]3.3}{[1][5][]5}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Alignment with Random Backpropagation Weights}{5}{section.4}}
\newlabel{sec:alignment}{{4}{5}{Alignment with Random Backpropagation Weights}{section.4}{}}
\newlabel{sec:alignment@cref}{{[section][4][]4}{[1][5][]5}}
\newlabel{def:alignment}{{4.1}{5}{}{theorem.4.1}{}}
\newlabel{def:alignment@cref}{{[definition][1][4]4.1}{[1][5][]5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Regularized feedback alignment}{6}{subsection.4.1}}
\newlabel{eqn:loss-with-reg}{{4.1}{6}{Regularized feedback alignment}{equation.4.1}{}}
\newlabel{eqn:loss-with-reg@cref}{{[equation][1][4]4.1}{[1][6][]6}}
\newlabel{thm:nonlinear_conv_reg}{{4.2}{6}{}{theorem.4.2}{}}
\newlabel{thm:nonlinear_conv_reg@cref}{{[theorem][2][4]4.2}{[1][6][]6}}
\newlabel{eq:conv_reg}{{4.2}{6}{}{equation.4.2}{}}
\newlabel{eq:conv_reg@cref}{{[equation][2][4]4.2}{[1][6][]6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Alignment analysis for linear networks}{6}{subsection.4.2}}
\newlabel{thm:lin_conv}{{4.3}{6}{}{theorem.4.3}{}}
\newlabel{thm:lin_conv@cref}{{[theorem][3][4]4.3}{[1][6][]6}}
\newlabel{eq:reg_error_bd}{{4.3}{6}{}{equation.4.3}{}}
\newlabel{eq:reg_error_bd@cref}{{[equation][3][4]4.3}{[1][6][]6}}
\newlabel{def:isom}{{4.4}{7}{$(\gamma , \eps )$-Isometry}{theorem.4.4}{}}
\newlabel{def:isom@cref}{{[definition][4][4]4.4}{[1][7][]7}}
\newlabel{prop:isom}{{4.5}{7}{}{theorem.4.5}{}}
\newlabel{prop:isom@cref}{{[proposition][5][4]4.5}{[1][7][]7}}
\newlabel{thm:lin_align}{{4.6}{7}{}{theorem.4.6}{}}
\newlabel{thm:lin_align@cref}{{[theorem][6][4]4.6}{[1][7][]7}}
\@writefile{toc}{\contentsline {paragraph}{Large sample scenario.}{7}{section*.2}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Simulations}{7}{section.5}}
\citation{wager2013dropout}
\citation{deng2012mnist}
\@writefile{toc}{\contentsline {paragraph}{Feedback alignment on synthetic data.}{8}{section*.3}}
\newlabel{fig:align_lr_non_autograd_l2}{{2a}{8}{Alignment on linear network.\relax }{figure.caption.4}{}}
\newlabel{fig:align_lr_non_autograd_l2@cref}{{[subfigure][1][2]2a}{[1][8][]8}}
\newlabel{sub@fig:align_lr_non_autograd_l2}{{a}{8}{Alignment on linear network.\relax }{figure.caption.4}{}}
\newlabel{sub@fig:align_lr_non_autograd_l2@cref}{{[subfigure][1][2]2a}{[1][8][]8}}
\newlabel{fig:align_nn_relu_autograd_l2}{{2b}{8}{Alignment on ReLU network.\relax }{figure.caption.4}{}}
\newlabel{fig:align_nn_relu_autograd_l2@cref}{{[subfigure][2][2]2b}{[1][8][]8}}
\newlabel{sub@fig:align_nn_relu_autograd_l2}{{b}{8}{Alignment on ReLU network.\relax }{figure.caption.4}{}}
\newlabel{sub@fig:align_nn_relu_autograd_l2@cref}{{[subfigure][2][2]2b}{[1][8][]8}}
\newlabel{fig:align_nn_tanh_autograd_l2}{{2c}{8}{Alignment on Tanh network.\relax }{figure.caption.4}{}}
\newlabel{fig:align_nn_tanh_autograd_l2@cref}{{[subfigure][3][2]2c}{[1][8][]8}}
\newlabel{sub@fig:align_nn_tanh_autograd_l2}{{c}{8}{Alignment on Tanh network.\relax }{figure.caption.4}{}}
\newlabel{sub@fig:align_nn_tanh_autograd_l2@cref}{{[subfigure][3][2]2c}{[1][8][]8}}
\newlabel{fig:loss_lr_non_autograd_l2}{{2d}{8}{Loss on linear network.\relax }{figure.caption.4}{}}
\newlabel{fig:loss_lr_non_autograd_l2@cref}{{[subfigure][4][2]2d}{[1][8][]8}}
\newlabel{sub@fig:loss_lr_non_autograd_l2}{{d}{8}{Loss on linear network.\relax }{figure.caption.4}{}}
\newlabel{sub@fig:loss_lr_non_autograd_l2@cref}{{[subfigure][4][2]2d}{[1][8][]8}}
\newlabel{fig:loss_nn_relu_autograd_l2}{{2e}{8}{Loss on ReLU network.\relax }{figure.caption.4}{}}
\newlabel{fig:loss_nn_relu_autograd_l2@cref}{{[subfigure][5][2]2e}{[1][8][]8}}
\newlabel{sub@fig:loss_nn_relu_autograd_l2}{{e}{8}{Loss on ReLU network.\relax }{figure.caption.4}{}}
\newlabel{sub@fig:loss_nn_relu_autograd_l2@cref}{{[subfigure][5][2]2e}{[1][8][]8}}
\newlabel{fig:loss_nn_tanh_autograd_l2}{{2f}{8}{Loss on Tanh network.\relax }{figure.caption.4}{}}
\newlabel{fig:loss_nn_tanh_autograd_l2@cref}{{[subfigure][6][2]2f}{[1][8][]8}}
\newlabel{sub@fig:loss_nn_tanh_autograd_l2}{{f}{8}{Loss on Tanh network.\relax }{figure.caption.4}{}}
\newlabel{sub@fig:loss_nn_tanh_autograd_l2@cref}{{[subfigure][6][2]2f}{[1][8][]8}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Comparisons of alignment and convergence for the feedback alignment algorithm with different levels of $\ell _2$ regularization. In \cref  {fig:align_lr_non_autograd_l2,fig:align_nn_relu_autograd_l2,fig:align_nn_tanh_autograd_l2}, the data points represent the mean value computed across simulations, and the error bars mark the standard deviation out of $50$ independent runs. In \cref  {fig:loss_lr_non_autograd_l2,fig:loss_nn_relu_autograd_l2,fig:loss_nn_tanh_autograd_l2}, we show the trajectories of the training loss for networks with $p = 3200$, with the shaded areas indicating the standard deviation over $50$ independent runs. The $x$-axes on the first row and the $y$-axes on the second row are presented using a logarithmic scale.\relax }}{8}{figure.caption.4}}
\newlabel{fig:synthetic-l2}{{2}{8}{Comparisons of alignment and convergence for the feedback alignment algorithm with different levels of $\ell _2$ regularization. In \cref {fig:align_lr_non_autograd_l2,fig:align_nn_relu_autograd_l2,fig:align_nn_tanh_autograd_l2}, the data points represent the mean value computed across simulations, and the error bars mark the standard deviation out of $50$ independent runs. In \cref {fig:loss_lr_non_autograd_l2,fig:loss_nn_relu_autograd_l2,fig:loss_nn_tanh_autograd_l2}, we show the trajectories of the training loss for networks with $p = 3200$, with the shaded areas indicating the standard deviation over $50$ independent runs. The $x$-axes on the first row and the $y$-axes on the second row are presented using a logarithmic scale.\relax }{figure.caption.4}{}}
\newlabel{fig:synthetic-l2@cref}{{[figure][2][]2}{[1][8][]8}}
\@writefile{toc}{\contentsline {paragraph}{Feedback alignment on the MNIST dataset.}{8}{section*.5}}
\citation{lillicrap2016random}
\citation{lillicrap2016random}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Comparisons on alignment and accuracy for feedback alignment algorithm with $\lambda =0,0.1,0.3$. The left figure shows alignment defined by $\qopname  \relax o{cos}\angle (\delta _\mathrm  {BP}(h),\delta _\mathrm  {FA}(h))$, and right figure shows the accuracy on the test set. The dashed lines and corresponding shaded areas represent the means and the standard deviations over $10$ runs with random initialization.\relax }}{9}{figure.caption.6}}
\newlabel{fig:mnist}{{3}{9}{Comparisons on alignment and accuracy for feedback alignment algorithm with $\lambda =0,0.1,0.3$. The left figure shows alignment defined by $\cos \angle (\dbp (h),\dfa (h))$, and right figure shows the accuracy on the test set. The dashed lines and corresponding shaded areas represent the means and the standard deviations over $10$ runs with random initialization.\relax }{figure.caption.6}{}}
\newlabel{fig:mnist@cref}{{[figure][3][]3}{[1][9][]9}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion}{9}{section.6}}
\bibstyle{apalike2}
\bibdata{feedback_alignment}
\bibcite{akrout}{{1}{2019}{{Akrout et~al.}}{{}}}
\bibcite{bartunov}{{2}{2018}{{Bartunov et~al.}}{{}}}
\bibcite{bellec}{{3}{2019}{{Bellec et~al.}}{{}}}
\bibcite{chen2020deep}{{4}{2020}{{Chen \& Xu}}{{}}}
\bibcite{deng2012mnist}{{5}{2012}{{Deng}}{{}}}
\bibcite{du2019gradient}{{6}{2019}{{Du et~al.}}{{}}}
\bibcite{du2018gradient}{{7}{2018}{{Du et~al.}}{{}}}
\bibcite{elman}{{8}{1996}{{Elman et~al.}}{{}}}
\bibcite{frenkel2021learning}{{9}{2021}{{Frenkel et~al.}}{{}}}
\bibcite{gao2020model}{{10}{2020}{{Gao \& Lafferty}}{{}}}
\bibcite{hebb1}{{11}{1961}{{Hebb}}{{}}}
\bibcite{jacot2018neural}{{12}{2018}{{Jacot et~al.}}{{}}}
\bibcite{launay2020direct}{{13}{2020}{{Launay et~al.}}{{}}}
\bibcite{lillicrap2016random}{{14}{2016}{{Lillicrap et~al.}}{{}}}
\bibcite{lillicrap2020backpropagation}{{15}{2020}{{Lillicrap et~al.}}{{}}}
\bibcite{medler}{{16}{1998}{{Medler}}{{}}}
\bibcite{nokland2016direct}{{17}{2016}{{N{\o }kland}}{{}}}
\bibcite{oja}{{18}{1982}{{Oja}}{{}}}
\bibcite{paulsen}{{19}{2000}{{Paulsen \& Sejnowski}}{{}}}
\bibcite{pdp}{{20}{1986a}{{Rumelhart et~al.}}{{}}}
\bibcite{rumelhart:86}{{21}{1986b}{{Rumelhart et~al.}}{{}}}
\bibcite{sejnowski2}{{22}{1999}{{Sejnowski}}{{}}}
\bibcite{sejnowski1}{{23}{1989}{{Sejnowski \& Tesauro}}{{}}}
\bibcite{wager2013dropout}{{24}{2013}{{Wager et~al.}}{{}}}
\bibcite{yamins2}{{25}{2016}{{Yamins \& DiCarlo}}{{}}}
\bibcite{ilker1}{{26}{2019}{{Yildirim et~al.}}{{}}}
