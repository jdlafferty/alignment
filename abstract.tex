\begin{abstract}
  Backpropagation has been one of the most important ideas that underlie the success of deep learning; however, some people have been arguing that backpropagation is not biologically plausible. One of the alternatives that have been proposed is Feedback Alignment, where random backpropagation weights instead of forward-connection weights are used for gradient descent. In this paper, we analyze the convergence of gradient descent with the random backpropagation weights for two-layer over-parameterized networks on regression. We prove that the squared error loss converges to zero in linear rate. In addition, we show that for networks without activation, the network parameters align to the random backpropagation weights when the parameters are regularized. Our simulations corroborate the theoretical analysis for convergence and alignment while further numerical results also confirm the alignment phenomenon on two-layer networks with non-linear activation when the parameters are regularized.
\end{abstract}