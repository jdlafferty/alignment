\begin{abstract}
Stochastic gradient descent with backpropagation is the workhorse of artificial neural networks. It has long been recognized that backpropagation fails to be a biologically plausible algorithm. Fundamentally, it is a ``non-local'' procedure---updating one neuron's  weights requires knowledge of the weights or receptive fields of distant neurons from deeper layers. This limits the use of artificial neural networks as a tool for understanding the biological principles of information processing in the brain. Lillicrap et al.~(2016) propose a more biologically plausible ``feedback alignment'' algorithm and show promising simulations. In this paper we study the mathematical properties of the feedback alignment procedure by analyzing convergence and alignment for two-layer overparameterized networks under squared error loss. In the linear case, we show convergence of the error at a linear rate, and prove that the parameters become aligned with the random backpropagation weights when regularization is used. In the nonlinear case, we show convergence of the error. Simulations are given that are consistent with this analysis and suggest further generalizations. These results provide a foundation for understanding the ability of certain local algorithms to carry out weight learning in a manner that is fundamentally different than Hebbian learning, while providing performance that is comparable with the full non-local backpropagation algorithm.
\end{abstract}
