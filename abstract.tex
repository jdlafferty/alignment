\begin{abstract}
Stochastic gradient descent with backpropagation is the workhorse of
artificial neural networks. It has long been recognized that
backpropagation fails to be a biologically plausible
algorithm. Fundamentally, it is a ``non-local'' procedure---updating
one neuron's  weights requires knowledge of the weights or receptive
fields of distant neurons from deeper layers. This limits the use of
artificial neural networks as a tool for understanding the biological
principles of information processing in the brain. Lillicrap et
al.~(2016) propose a more biologically plausible ``feedback
alignment'' algorithm and show promising simulations. In this paper we
study the mathematical properties of the feedback alignment procedure
by analyzing convergence and alignment for two-layer overparameterized
networks under squared error loss. In the overparameterized setting,
we prove that the error converges exponentially fast, also that
regularization is necessary in order for the  parameters to become aligned with the random backpropagation weights. Simulations are given that are consistent with this analysis and suggest further generalizations. These results provide a foundation for understanding the ability of certain local algorithms to carry out weight learning in a manner that is fundamentally different than Hebbian learning, while providing performance that is comparable with the full non-local backpropagation algorithm.
\end{abstract}
