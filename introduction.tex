\section{Introduction}

The roots of artificial neural networks draw inspiration from networks of biological neurons in the brain \citep{pdp,pinker,elman,medler}. Grounded in simple abstractions of membrane potentials and firing, neural networks are increasingly being employed as a computational tool for better understanding the biological principles of information processing in the brain \citep{ilker1,ilker2,yamins1,yamins2}. Even when full biological fidelity is not required, it can be useful to better align the computational abstraction with neuroscience principles.

From early days to the present, stochastic gradient descent has been the workhorse of artificial neural networks. Conveniently, calculation of gradients can be carried out using the backpropagation algorithm, where reverse mode automatic differentiation provides a powerful way of computing the derivatives for general architectures \citep{rumelhart:86}.
It has long been recognized that backpropagation fails to be a biologically plausible algorithm. Fundamentally, it is a non-local procedure---updating the weight between a presynaptic and postsynaptic neuron requires knowledge of the weights between the postsynaptic neuron and other neurons. No known biological mechanism exists for propagating information in this manner. This limits the use of artificial neural networks as a tool for understanding learning in the brain.

A wide range of approaches can be explored as a potential basis for learning and synaptic plasticity. Hebbian learning is a classical procedure for adjusting weights where
the repeated stimulation by a presynaptic neuron that results in the subsequent
firing of the postsynapic neuron will result in an increased strength in the connection
between the two cells \citep{hebb1,paulsen}. Several variants of Hebbian learning, some making connections to principal components analysis, have been proposed
% \citep{oja,sejnowski1,sejnowski2}. In this paper, our focus  is on a formulation of \cite{lillicrap2016random} which uses random
backpropagation weights that are fixed during the learning process.
Related proposals have been made in a series recent papers \citep{akrout,bellec,lillicrap2020backpropagation}
The use of random feedback weights, which are not directly tied to the forward weights, removes issues of non-locality. However, it is not clear under what conditions optimization of error and learning can be successful. \citep{lillicrap2016random} give suggestive simulations, and some analysis that shows that ????. However, it has thus been an open problem to explain behavior of this algorithm as a basis for training the weights of a neural network.

In this paper we study the mathematical properties of the feedback alignment procedure by analyzing convergence and alignment for two-layer networks under squared error loss. In the overparameterized setting, we prove that the error converges to zero exponentially fast, and also that regularization is necessary in order for the  parameters to become aligned with the random backpropagation weights. Simulations are given that are consistent with this analysis and suggest further generalizations. These results contribute to our understanding of how biologically plausible algorithms might carry out weight learning in a manner different from Hebbian learning, with performance that is comparable with the full non-local backpropagation algorithm.

 %% statement of results here?



In the following Section we state the problem in more detail, and dive more deeply into previous   work on topic.
