%!TEX root=./main.tex

\section{Alignment of Parameters to Random Backpropagation Weights}\label{sec:alignment}

The most prominent characteristic of feedback alignment algorithm is its phenomenon of alignment between the error signals back-propagated with forward weights and those with fixed random backward weights during training. Specifically, if we denote $h\in \R^p$ to be the hidden layer of the network, then we write $\dbp(h) \defeq \frac{\partial \Loss}{\partial h}$ to represent the error signals with respect to the hidden layer that are backpropagated with the feed-forward weights and $\dfa(h)$ as the error signals computed with fixed random backward weights.
\citet{lillicrap2016random} notice a decreasing angle between $\dbp(h)$ and $\dfa(h)$ during training, which suggests that feedback alignment algorithm can still carry gradient information back through layers even the signals are backpropagated through random weights.
In particular, the error signals $\dbp(h)$ and $\dfa(h)$ for the two-layer network \eqref{eqn:nonlinear-network} are given by 
\begin{equation}
    \dbp(h) = \frac{1}{\sqrt{p}}\beta\sum_{i=1}^ne_i \quad\text{and}\quad \dfa(h) = \frac{1}{\sqrt{p}}b\sum_{i=1}^ne_i,
\end{equation}
and the alignment can be characterized by the angle between $\beta(t)$ and $b$ during training. We formalize this concept of alignment by the following definition.
\begin{definition}\label{def:alignment}
    A series of vectors $\{\beta(t)\}_{t=0}^\infty$ align with a fixed vector $b$ if there exists a constant $c>0$ and time $T_c$ such that  $\cos\angle(b, \beta(t)) = \frac{\langle b, \beta(t)\rangle}{\|b\|\|\beta(t)\|} \geq c$ for all $t > T_c$.
\end{definition}

\subsection{Regularized Feedback Alignment}
Unfortunately, this alignment between $\beta(t)$ and $b$ is not guaranteed for over-parameterized networks and the loss \eqref{eqn:squared-loss}. To further illustrate our point, we control the cosine value of the angle by inequalities \eqref{eq:weights} from \cref{thm:nonliner_conv}, \ie,
\begin{equation}
    \Big|\cos\angle(b, \beta(t))\Big| \leq \frac{|\langle \frac{b}{\|b\|}, \beta(0)\rangle|+ \|\beta(t)- \beta(0)\|}{\|\beta(0)\|-\|\beta(t)-\beta(0)\|} = O\big(\frac{n}{\sqrt p}\big),
\end{equation}
which indicates that $\beta(t)$ and $b$ turn orthogonal as the network becomes wider. Intuitively, this can be understood as the parameters stay near their initializations during training when $p$ is large, where $\beta(0)$ and $b$ are almost orthogonal with each other. This motivates us to regularize the network parameters. Especially, we consider in this work the squared error loss with a $\ell_2$ regularization term on $\beta$:
\begin{equation}
\label{eqn:loss-with-reg}
\Loss(t, W, \beta) = \frac{1}{2}\sum_{i=1}^n\big(f(x_i)-y_i\big)^2 + \frac{1}{2}\lambda(t)\|\beta\|^2,
\end{equation}
where $\{\lambda(t)\}_{t=0}^\infty$ is a sequence of regularization rates, which defines a series of loss functions for different training steps $t$. The feedback alignment algorithm under the regularized loss are summarized in Algorithm \ref{algo:fa-reg}. Compared to \cref{algo:fa}, an extra contraction factor $1-\lambda(t)$ has been added in the update of $\beta(t)$, which doesn't affect the locality of the algorithm but helps the alignment by shrinking the component of $\beta(0)$ in $\beta(t)$.

%\begin{minipage}{\textwidth}
\begin{algorithm}[H]
\centering
\caption{Regularized Feedback Alignment on Two-Layer Networks}\label{algo:fa-reg}
    \begin{algorithmic}[1]
        \Require activation $\psi$, dataset $\{(x_i,y_i)\}_{i=1}^n$, and step size $\eta$, and regularization $\{\lambda(t)\}_t$.
        \State {\bf initialize} $W(0)$, $\beta(0)$ and $b$ with standard Gaussian entries
        \While{not converge}
            \State $\beta_r(t+1) \gets (1-\lambda(t))\beta_r(t) - \frac{\eta}{\sqrt p} \sum_{i=1}^n e_i(t)\psi(w_r(t)\transpose x_i)$ for $r\in[p]$
            \State $w_r(t+1) \gets w_r(t) - \frac{\eta}{\sqrt{p}} \sum_{i=1}^n e_i(t) b_r\psi'(w_r(t)\transpose x_i)x_i$ for $r\in[p]$
        \EndWhile
    \end{algorithmic}    
\end{algorithm}
%\end{minipage}

\subsection{Theoretical Results on Linear Networks}
In this section, we only focus on the theoretical analysis of alignment for linear networks, which is equivalent to setting the activation function $\psi$ into an identity map. Before presenting our results on alignment, we first provide an upper bound for the prediction error $e(t)$, which forms the cornerstone for our proofs. \cref{thm:lin_conv} shows that the error can be upper bounded if $X = (x_1,\ldots,x_n)\transpose$ has full row rank and the summation of $\lambda(t)$ is upper bounded by $S_\lambda = O(\sqrt{p/n})$.

\begin{theorem}
\label{thm:lin_conv}
Assume \textnormal{(1)}  $\|y\| = \Theta(\sqrt n)$, $\lambda_{\min}(XX\transpose)>\gamma$ and $\lambda_{\max}(XX\transpose)<M$ for some constants $M>\gamma>0$, \textnormal{(2)} $\sum_{t=0}^\infty \lambda(t) \leq  S_\lambda = c_{S}\frac{\gamma\sqrt{\gamma p}}{\eta\sqrt{n}M}$ for some constant $c_{S}$, \textnormal{(3)} $\eta \leq 1/(\max \lambda(t)+M)$.
For any $\delta\in(0,1)$, if $p = \Omega(\frac{Md\log(d/\delta)}{\gamma})$, the following inequality holds for all $t\geq 0$ with probability at least $1-\delta$
\begin{equation}
\label{eq:reg_error_bd}
\|e(t+1)\|\leq \big(1-\frac{\eta\gamma}{2}-\eta\lambda(t)\big)\|e(t)\| + \lambda(t)\|y\|.
\end{equation} 
\end{theorem}

We remark here some connections between \cref{thm:lin_conv} and \cref{thm:nonliner_conv}. In the linear case, the kernel matrix $G$ reduces to the form $X W\transpose W X\transpose$ and its expectation $\overline{G}$ at initialization also reduces to $X X\transpose$, so \cref{assump:G} holds if $XX\transpose$ is positive definite, which is equivalent to $x_i$'s being linearly independent. Since regularization terms $\lambda(t)$ make additional contributions to the error $e(t)$ as well as the kernel matrix $G(t)$, an upper bound on $\sum_{t\geq 0}\lambda(t)$ is needed to ensure the positiveness of the minimal eigenvalue of $G(t)$ during training, such that the error $e(t)$ can be controlled. In particular, if there is no regularization, \ie, $\lambda(t)=0$ for all $t\geq 0$, then we recover exponential convergence for the error $\|e(t)\|$ as in \cref{thm:nonliner_conv}.

Our results on alignment also rely on an isometric condition on $X$, which requires the minimum and the maximum eigenvalues of $XX\transpose$ to be close enough (\cf \cref{def:isom}). On the other hand, this condition is relatively mild and can be satisfied when $X$ has random Gaussian entries with a gentle dimensional constraint, as demonstrated by \cref{prop:isom}. Finally, we show in \cref{thm:lin_align} that under a simple regularization strategy where a constant regularization is adopted until a cutoff time $T$, regularized feedback alignment achieves alignment if $X$ satisfies the isometric condition.
{}
\begin{definition}[$(\gamma, \eps)$-Isometric]
\label{def:isom}
Given positive constant $\gamma$ and $\eps$, we say $X$ is $(\gamma, \eps)$-isometric if 
$\lambda_{\min}(XX\transpose)\geq\gamma$ and $\lambda_{\max}(XX\transpose)\leq(1+\eps)\gamma$.
\end{definition}

\begin{proposition}
\label{prop:isom}
Assume $X\in\R^{n\times d}$ has independent entries drawn from $N(0,1/d)$. For any $\eps \in (0,1/2)$ and $\delta \in (0,1)$, if $d=\Omega(\frac{1}{\eps}\log\frac{n}{\delta}+\frac{n}{\eps}\log \frac{1}{\eps})$, $X$ is $(1-\eps, 4\eps)$-isometric with probability $1-\delta$.
\end{proposition}

\begin{theorem}
\label{thm:lin_align}
Assume all conditions from \cref{thm:lin_conv} hold and $X$ is $(\gamma, \eps)$-isometric with a small constant $\eps$. Let the regularization weights
\begin{align*}
\lambda(t) = 
\begin{cases}
    \lambda, \quad t\leq T,\\
    0, \quad t > T,
\end{cases}
\end{align*}
with $\lambda=K\gamma$ and $T = \lfloor S_\lambda/\lambda\rfloor$ for some large constant $K$. Then, for any $\delta\in(0,1)$, if $p = \Omega(d\log(d/\delta))$, with probability at least $1-\delta$, regularized feedback alignment on linear network achieves alignment. Specifically, there exist a positive constant $c=c_\delta$ and time $T_c$, such that $\cos\angle(b, \beta(t))\geq c$ for all $t>T_c$.
\end{theorem}

We defer the proofs of \cref{prop:isom}, \cref{thm:lin_conv} and \cref{thm:lin_align} to \cref{sec:appendix-alignment}. In fact, we prove \cref{thm:lin_align} by directly computing $\beta(t)$ and the cosine of the angle. Although $b$ doesn't show up in the update of $\beta$, it could still propagate to $\beta$ through $W$. Since the amount of component of $b$ in $\beta(t)$ depends on the inner-product $\langle e(t), e(t')\rangle$ for all previous steps $t'\leq t$, the norm bound \eqref{eq:reg_error_bd} from \cref{thm:lin_conv} is insufficient so a more careful study on $e(t)$ is provided to accomplish the proof. We should point out that the constant $c$ in the lower bound is independent of the sample size $n$, input dimension $d$, network width $p$ and learning rate $\eta$. We also remark that the cutoff schedule of $\lambda(t)$ is just chosen for simplicity of the proof. For other schedules such as inverse-squared decay or exponential decay, one could also obtain the same alignment result as long as the summation of $\lambda(t)$ is less than $S_\lambda$.

\paragraph{Large sample scenario.} In \cref{thm:lin_conv,thm:lin_align}, we consider the case where sample size $n$ is less than input dimension $d$ so that the positive definiteness of $XX\transpose$ can be established. However, both results still hold for $n>d$. In fact, the squared error loss $\calL$ can be written as
\begin{equation*}
\sum_{i=1}^n\big(f(x_i)-y\big)^2 = \big\|\frac{1}{\sqrt{p}}XW\transpose\beta-y\big\|^2 = \big\|\frac{1}{\sqrt{p}}XW\transpose\beta-\bar{y}\big\|^2+ \|\bar{y}-y\|^2,
\end{equation*}
where $\bar{y}$ denotes the projection of $y$ onto the column space of $X$. Without loss of generality, we assume $y=\bar{y}$. As a result, $y$ and columns of $X$ are all in the same $d$-dimensional subspace of $\Rn$ and $XX\transpose$ is positive definite on this subspace as long as $X$ has full column rank. Consequently, we can either work on this subspace of $\Rn$ or project all the vectors onto $\Rd$, and the isometric condition are revised to only consider the $d$ nonzero eigenvalues of $XX\transpose$.
% with a left multiplication of $X\transpose$. (ganlin: actually U^T)


