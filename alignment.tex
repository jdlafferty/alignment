\section{Alignment of Parameters to Random Backpropagation Weights: Linear Case}\label{sec:alignment}

In this section we consider the process of training a two-layer linear network with feedback alignment on linear regression data; we analyze its rate of convergence, and more importantly, the alignment of the feed-forward weights $\beta$ with the backward weights $b$ as a result of the training, which is a defining characteristic of the algorithm \citep{lillicrap2016random}. In particular, we will show that regularization acts as a vital factor that leads to alignment and replacing the backpropagation weights alone is not able to achieve alignment for over-parametrized networks.

Under the linear setup where the activation $\psi$ is identity function, we recover the corresponding two-layer linear network as 
\begin{align}\label{eqn:linear-network}
    f(X) = \frac{1}{\sqrt p}XW\transpose\beta.
\end{align}
Here we consider the squared error loss with a $L_2$ regularization term on $\beta(t)$, where $\{\lambda(t)\}_{t=0}^\infty$ forms a series of regularization rate $\lambda(t)$:
\begin{equation}\label{eqn:loss-with-reg}
    L(t) = \frac{1}{2}\|f(t) - y\|^2 + \frac{\lambda(t)}{2}\|\beta(t)\|^2.
\end{equation}
The series of $\lambda(t)$ thus defines a series of loss functions that governs the learning process. If we set $\lambda(t) = 0$ for all $t$, then we fall back to the case before; however, we are going to show the importance of regularization to feedback alignment in achieving convergence and alignment simultaneously. Due to the lack of non-linearity, $f(t)$ is always going to fall into the range of $X$, and the portion of $y$ orthogonal to $\range(X)$ cannot be fitted by the model. If we decompose the loss into $L(t) = \frac{1}{2}\|f(t) - \bar y\|^2 + \frac{1}{2}\|y - \bar y\|^2 + \frac{\lambda(t)}{2}\|\beta(t)\|^2$, where $\bar y$ denotes the projection of $y$ onto the column space of $X$ and $y-\bar y$ is the component orthogonal to the column space, it is not hard to find that $\|y - \bar y\|^2$ remains constant during training. Hence, without loss of generality, we assume $y$ lying in the column space of $X$ and consider $L(t) = \frac{1}{2}\|f(t) - \bar y\|^2 + \frac{\lambda(t)}{2}\|\beta(t)\|^2$. 
% In a teacher-student model, we assume a teacher function $f_*(X)$ with $W_*$ and $\beta_*$ as its first- and second-layer weights, respectively. In this manuscript, we further assume $W_*$ to be a random Gaussian matrix fixed in advance, where $(W_*)_{ij}\sim\calN(0,1)$ independently for all $i\in[p]$ and $j\in[d]$. Similarly, $\beta_*$ is also considered to be a random Gaussian vector with independent Gaussian entries. Therefore, for any training data $X$, the teacher function $f_*$ provides the corresponding training labels $y = f_*(X)\in\Rn$ to the student.

For linear networks, the feedback alignment updates on $W(t)$ and $\beta(t)$ with respect to loss \eqref{eqn:loss-with-reg} can be presented as
\begin{align}\label{eqn:update-reg}
    W(t+1) = W(t) - \frac{\eta}{\sqrt p}b e(t)\transpose X, \qquad \beta(t+1) = (1-\eta\lambda(t))\beta(t) - \frac{\eta}{\sqrt p} W(t)X\transpose e(t), 
\end{align}
where the contraction of $\beta(t)$ at each step is due to the regularization term $\frac{\lambda(t)}{2}\|\beta(t)\|^2$ in \eqref{eqn:loss-with-reg}. This regularized version of the algorithm is summarized in \cref{algo:fa-reg}, and we consider only the linear case in our theoretical analysis.
% Under these assumptions, we are trying to show the convergence of the prediction error $\|f(t) - y\|^2$. 

\begin{definition}\label{def:alignment}
    A series of vectors $\{\beta(t)\}_{t=0}^\infty$ align with a fixed vector $b$ if there exists a constant $c>0$ and time $T_c$ such that for all $t > T_c$ it holds
    \begin{align*}
        \cos\angle(b, \beta(t)) = \frac{b\transpose \beta(t)}{\|b\|\|\beta(t)\|} \geq c.
    \end{align*}
\end{definition}

\begin{definition}[$(\gamma, \epsilon)$-Isometric]
Given positive constant $\gamma$ and $\epsilon$, we say $X$ is $(\gamma, \epsilon)$-isometric if 
$\lambda_{\min}(XX\transpose)\geq\gamma$ and $\lambda_{\max}(XX\transpose)\leq(1+\epsilon)\gamma$.
\end{definition}

We show the convergence of feedback alignment when $\sum_{t=1}^\infty \lambda(t)$ converges. 
\begin{theorem}
\label{thm:lin_conv}
Assume
\begin{enumerate}
\item $\|y\| = \Theta(\sqrt n)$. $\lambda_{\min}(XX\transpose)>\gamma$ and $\lambda_{\max}(XX\transpose)<M$ for some constants $M>\gamma>0$,
\item $\sum_{t=0}^\infty \lambda(t) \leq  S_\lambda = c_{S}\frac{\gamma\sqrt{\gamma p}}{\eta\sqrt{n}M}$ for some constant $c_{S}$,
\item $\eta \leq 1/(\max \lambda(t)+M)$.
\end{enumerate}
For any $\delta\in(0,1)$, if $p = \Omega(\frac{Md\log(d/\delta)}{\gamma})$, the following inequality holds for all $t\geq 0$ with probability at least $1-\delta$
\begin{equation}
\label{eq:reg_error_bd}
\|e(t+1)\|\leq \big(1-\frac{\eta\gamma}{2}-\eta\lambda(t)\big)\|e(t)\| + \lambda(t)\|y\|
\end{equation} 
\end{theorem}

\begin{theorem}[informal]
    If we assume $p$ and $\frac{n}{d}$ are sufficiently large and $\sum_{t=1}^\infty \lambda(t)$ is bounded, then the error $\|f(t) - y\|^2$ under feedback alignment based on loss function \eqref{eqn:loss-with-reg} achieves an $O(1/t)$ convergence rate.
\end{theorem}
In particular, if there is no regularization, \ie, $\lambda(t)=0$ for all $t\geq 0$, then we show a linear convergence rate for the error $\|f(t) - y\|^2$.
\begin{corollary}[informal]
    If we assume $p$ and $\frac{n}{d}$ are sufficiently large and $\lambda(t) = 0$ for all $t\geq 0$, the error $\|f(t) - y\|^2$ under feedback alignment on \eqref{eqn:loss-with-reg} achieves linear convergence rate. In particular, $\|f(t) - y\| \leq \frac{1+\eps}{1-\eps}(1-\frac{n\eta}{2d})^t \|f(0) - y\|$.
\end{corollary}
Finally, we show that under a simple regularization strategy where a constant regularization is adopted until a cutoff time $T$, feedback alignment under loss function \eqref{eqn:loss-with-reg} achieves alignment asymptotically.
\begin{theorem}
\label{thm:lin_align}
Assume all conditions from Theorem \ref{thm:lin_conv} hold and $X$ is $(\gamma, \epsilon)$-isometric with a small constant $\epsilon$. Let the regularization weights
\begin{align*}
\lambda(t) = 
\begin{cases}
    \lambda, \quad t\leq T,\\
    0, \quad t > T,
\end{cases}
\end{align*}
with $\lambda=\Omega(\gamma)$ and $T = \lfloor S_\lambda/\lambda\rfloor$. Then, for any $\delta\in(0,1)$, if $p = \Omega(d\log(d/\delta))$, with probability at least $1-\delta$, feedback alignment on linear network achieves alignment. Specifically, there exist a constant $c=c_\delta$ and time $T_c$, such that $\cos\angle(b, \beta(t))\geq c$ for all $t>T_c$.
\end{theorem}


