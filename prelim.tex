\section{Preliminaries}

In this manuscript, we consider a two-layer network with the hidden-layer width $p$, and for any input dataset $\{x_i\}_{i=1}^n$ with $n$ samples and $x_i\in\Rd$ for $i\in[n]$, the network $f$ outputs 
\begin{align}\label{eqn:nonlinear-network}
    f(X) = \frac{1}{\sqrt p}\psi(XW\transpose)\beta \in\Rn,
\end{align}
where $\psi$ denotes the element-wise activation function, $W\in\Rpd$ denotes the first-layer forward-connection weights, $\beta\in\Rp$ represents the second-layer weights, and $X\in\Rnd$ is the design matrix formed by concatenating the training samples $x_i$'s as row vectors. 

We consider in this work a linear regression problem. The label $y = X\zeta + \epsilon$ is generated by the design matrix $X$ and a parameter vector $\zeta\in\Rd$, where $\epsilon\in\Rn$ is a Gaussian vector with each entry $\epsilon_i\sim\calN(0,\sigma^2)$ independently. Without loss of generality, we consider a normalized label $y$, where $|y_i|\leq 1$ for all $i\in[n]$. 
\chris{the constructive definition with $\epsilon$ may seem redundant in our settings}
We focus especially on a simple quadratic loss $L = \frac{1}{2}\|f(X)-y\|^2$. 

\paragraph{Backpropagation on two-layer network.}

The usual backpropagation scheme updates the weight parameters following gradient direction. In particular, for two-layer network the update rules on $W$ and $\beta$ write $W_{t+1} = W_t - \eta \frac{\partial L}{\partial W_t}$ and $\beta_{t+1} = \beta_{t} - \eta \frac{\partial L}{\partial \beta_t}$, where $\eta$ denotes the learning rate for gradient descent. The gradient information for weight parameters not only includes the backpropagated error signals but also consists of non-local information on weight parameters from top layers. More specifically, the gradient on the second-layer weights $\frac{\partial L}{\partial \beta} = \frac{1}{\sqrt{p}} \psi(W X\transpose) (f(X) - y)$ contains only the second-layer inputs $\psi(W X\transpose)$ and error signal $f(X) - y$, while the gradient on the first-layer weights 
\begin{align}\label{eqn:backprop-gradient}
    \frac{\partial L}{\partial W} = \frac{1}{\sqrt{p}} [(\beta (f(X) - y)\transpose)\odot \psi'(WX\transpose)] X,
\end{align}
requires not only the local input $X$, local output $WX\transpose$, and error signal $f(X) - y$, but also the information on the second-layer weights $\beta$.
This requires the feed-forward connections between layers to be identical with the corresponding backward connections, and this requirement on synaptic symmetry is regarded as not biological plausible \citep{lillicrap2016random}. 

\paragraph{Feedback alignment on two-layer network.}

One of the alternative approaches proposed to improve the biological plausibility over backpropagation is \emph{Feedback Alignment}, which replaces the feed-forward weights by an independent fixed random backpropagation weights at each layer as backward weights \citep{lillicrap2016random}. In particular, for two-layer networks considered in this work, the second-layer weights $\beta$ is replaced with a fixed random backpropagation weights $b\in\Rp$ at the gradient update while the gradient update on $\beta$ remains the same. By replacing the $\beta$ vector with $b$ in \cref{eqn:backprop-gradient}, the gradient update for $W$ becomes
\begin{align}\label{eqn:alignment-update}
    W_{t+1} = W_t - \frac{\eta}{\sqrt{p}} [(b (f(X) - y)\transpose)\odot \psi'(WX\transpose)] X.
\end{align}
After the substitution, the backpropagation update on $W$ no longer depends on $\beta$ directly and there is no requirement on synaptic symmetry. \cref{algo:fa} shows the feedback alignment algorithm, where $\psi'$ is applied entry-wisely on the matrix and the shorthand $e_t \defeq f_t(X) -y$ denotes the error made by the network $f$ in predicting $y$. \chris{shall we still call Feedback Alignment ``feedback alignment'' even if it fails to align?} Note that the dynamics of $\beta$ also do not follow the gradient flow path anymore considering its joint dynamics with $W$. Even though we originally start from a loss function $L$ and the gradient descent dynamics on such function, we end up arriving at this dynamics for feedback alignment that do not directly relate to any loss function.

\begin{minipage}{0.49\textwidth}
\begin{algorithm}[H]
\centering
\caption{Feedback Alignment}\label{algo:fa}
    \begin{algorithmic}[1]
        \Require Network $f$ with weight parameters $W,\beta$, activation function $\psi$, dataset $(X,y)$, learning rate $\eta$.
        \State {\bf initialize} $W_0$ and $\beta_0$ as Gaussian
        \While{$\|f_t(X) - y\|^2$ not converge}
            \State $W_{t+1} \gets W_t - \frac{\eta}{\sqrt p}[(b e_t\transpose)\odot \psi'(W_tX\transpose)] X$
            \State $\beta_{t+1} \gets \beta_t - \frac{\eta}{\sqrt p} \psi(W_tX\transpose) e_t$
        \EndWhile
        \State \Return $f$
    \end{algorithmic}    
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}{0.49\textwidth}
\begin{algorithm}[H]
\centering
\caption{Regularized Feedback Alignment}\label{algo:fa-reg}
    \begin{algorithmic}[1]
        \Require Network $f$ with weight parameters $W,\beta$, activation function $\psi$, dataset $(X,y)$, learning rate $\eta$, regularization rate $\{\lambda_t\}_t$.
        \State {\bf initialize} $W_0$ and $\beta_0$ as Gaussian
        \While{$\|f_t(X) - y\|^2$ not converge}
            \State $W_{t+1} \gets W_t - \frac{\eta}{\sqrt p}[(b e_t\transpose)\odot \psi'(W_tX\transpose)] X$
            \State $\beta_{t+1} \gets (1-\eta\lambda_t)\beta_t - \frac{\eta}{\sqrt p} \psi(W_tX\transpose) e_t$
        \EndWhile
        \State \Return $f$
    \end{algorithmic}    
\end{algorithm}
\end{minipage}

Feedback alignment is named after its phenomenon of alignment between the feed-forward weights and the fixed backpropagation weights during training. In the case of two-layer network, the alignment is represented as the positive inner product between two vectors $\beta$ and $b$. More specifically, we define alignment as the following.
\begin{definition}\label{def:alignment}
    A series of vectors $\{\beta_t\}_{t=0}^\infty$ align with a fixed vector $b$ if there exists a positive constant $c$ and time $T_c$ such that for all $t > T_c$ it holds
    \begin{align*}
        \cos\angle(b, \beta_t) = \frac{b\transpose \beta_t}{\|b\|\|\beta_t\|} \geq c.
    \end{align*}
\end{definition}