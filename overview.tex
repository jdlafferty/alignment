%!TEX root=./main.tex
\section{Problem Statement and Overview of Results}

In this section we provide a formulation of the backpropagation
algorithm to establish notation and the context for our analysis. We then
formulate the feedback aligment algorithm that uses random backpropation weights.
A high-level overview of our results is then presented, together with
some of the intuition and proof techniques behind these results; we also contrast with what was known previously.

We mainly consider two-layer neural networks in the regression setting, specified by a family of functions  $f:\Rd \to \RR$ with input dimension $d$, sample size $n$, and $p$ neurons in the hidden layer. For an input $x\in\Rd$, the network outputs
\begin{align}\label{eqn:nonlinear-network}
    f(x) = \frac{1}{\sqrt p}\sum_{r=1}^p\beta_r\psi(w_r\transpose x)= \frac{1}{\sqrt p}\beta\transpose\psi(Wx),
\end{align}
where $W = (w_1,...,w_p)\transpose\in\Rpd$ and $\beta = (\beta_1,...,\beta_p)\transpose\in\Rp$ represent the feed-forward weights in the first and second layers, and $\psi$ denotes an element-wise activation function. The scaling by $\sqrt{p}$ is simply for convenience in the analysis.

Given $n$ input-response pairs $\{(x_i,y_i)\}_{i=1}^n$, the training objective is to minimize the mean squared error
\begin{equation}\label{eqn:squared-loss}
    \Loss(W,\beta) = \frac{1}{2n}\sum_{i=1}^n \big(y_i - f(x_i)\big)^2.
\end{equation}
Standard gradient descent attempts to minimize \eqref{eqn:nonlinear-network} by updating the feed-forward weights following gradient directions according to
\begin{align*}
    \beta_r(t+1) &= \beta_r(t)-\eta\frac{\partial\Loss}{\partial \beta_r}(W(t),\beta(t)) \quad\\ w_r(t+1) &= w_r(t)-\eta\frac{\partial\Loss}{\partial w_r}(W(t),\beta(t)),
\end{align*}
for each $r\in[p]$, where $\eta>0$ denotes the step size. We initialize $\beta(0)$ and $w_r(0)$  standard Gaussian vectors. We introduce the notation $f(t), e(t)\in \R^n$, with $f_i(t) = f(x_i)$ denoting the network output on input $x_i$ when the weights are $W(t)$ and $\beta(t)$, and $e_i(t) = y_i-f_i(t)$ denoting the corresponding prediction error or residual. With this notation,
the gradients are expressed as
\begin{equation*}
    \frac{\partial\Loss}{\partial \beta_r} = \frac{1}{\sqrt p}\sum_{i=1}^n e_i\psi(w_r\transpose x_i), \quad
    \frac{\partial\Loss}{\partial w_r} = \frac{1}{\sqrt p} \sum_{i=1}^n e_i \beta_r\psi'(w_r\transpose x_i)x_i.
\end{equation*}
Here it is seen that the the gradient of the first-layer weights $\frac{\partial \Loss}{\partial w_r}$ involves not only the local input $x_i$ and the change in
the response of the $r$-th neuron, but also the backpropagated error signal $e_i\beta_r$.
The appearance of $\beta_r$ is, of course, due to the chain rule; but in effect it requires that the forward weights between layers are identical to the backward weights under error propagation. A sketch of this is shown in Figure~\ref{fig:algo}; there is no evidence of biological mechanisms
that would enable such ``synaptic symmetry.''

\begin{figure*}[t]
  \begin{tabular}{cc}
    \hskip-20pt
    \includegraphics[width=.55\textwidth]{fig/fasketch}&\\[-1.8in]
    &
    \hskip-5pt
    \begin{minipage}{.47\textwidth}
    \begin{algorithm}[H]
    \centering
    \caption{Feedback Alignment}\label{algo:fa}
        \begin{algorithmic}[1]
            \Require Dataset $\{(x_i,y_i)\}_{i=1}^n$, step size $\eta$
            \State {\bf initialize} $W$, $\beta$ and $b$ as Gaussian
            \While{not converged}
                \State $\beta_r \gets \beta_r - \frac{\eta}{\sqrt p} \sum_{i=1}^n e_i \psi(w_r\transpose x_i)$
                \State $w_r \gets w_r - \frac{\eta}{\sqrt{p}} \sum_{i=1}^n e_i b_r\psi'(w_r\transpose x_i)x_i$
                \State for $r\in[p]$
            \EndWhile
        \end{algorithmic}
    \end{algorithm}%
    \end{minipage}
    \\[1.05in]

  \end{tabular}
\caption{Standard backpropagation updates the first layer weights for a hidden node $r$ with the second layer feedforward weight $\beta_r$. We study the procedure where the error is backpropagated instead using a fixed, random weight $b_r$.}
\label{fig:algo}
\end{figure*}


In the \textit{feedback alignment} procedure of \citep{lillicrap2016random},
when updating the weights $w_r$, the error signal is weighted not by the second layer feedforward weights $\beta$, but rather by a random set of weights $b\in\reals^p$ that are
fixed during the course of training. Equivalently, the gradients for the first layer are
replaced by the terms
\begin{align}\label{eqn:alignment-update}
  \widetilde{\frac{\partial\Loss}{\partial w_r}}   = \frac{1}{\sqrt{p}} \sum_{i=1}^n e_i b_r\psi'(w_r\transpose x_i)x_i.
\end{align}
Note, however, that this update rule does not correspond to the gradient with
respect to a modified loss function. The use of a random weight $b_r$ when updating
the first layer weights $w_r$ does not violate locality, and could conceivably be implemented by biological mechanisms; we refer to \cite{lillicrap2016random,bartunov,lillicrap2020backpropagation} for further discussion. A schematic of the relationship between the two algorithms is shown in Figure~\ref{fig:algo}.
