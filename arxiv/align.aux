\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{pdp,elman,medler}
\citation{ilker1}
\citation{yamins2}
\citation{rumelhart:86}
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\@newglossary{acronym}{alg}{acr}{acn}
\gdef\hy@title{Convergence and Alignment of Gradient Descent with Random Backpropagation Weights}
\gdef\hy@author{}
\gdef\hy@subject{}
\gdef\hy@keywords{}
\gdef\author@num{0}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{hebb1,paulsen}
\citation{oja,sejnowski1,sejnowski2}
\citation{lillicrap2016random}
\citation{akrout,bellec,lillicrap2020backpropagation}
\citation{bartunov}
\citation{lillicrap2016random}
\@writefile{toc}{\contentsline {section}{\numberline {2}Problem Statement and Overview of Results}{2}{section.2}}
\newlabel{eqn:nonlinear-network}{{2.1}{2}{Problem Statement and Overview of Results}{equation.2.1}{}}
\newlabel{eqn:nonlinear-network@cref}{{[equation][1][2]2.1}{[1][2][]2}}
\citation{lillicrap2016random}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Feedback Alignment\relax }}{3}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{algo:fa}{{1}{3}{Feedback Alignment\relax }{figure.caption.1}{}}
\newlabel{algo:fa@cref}{{[algorithm][1][]1}{[1][3][]3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Standard backpropagation updates the first layer weights for a hidden node $r$ with the second layer feedforward weight $\beta _r$. We study the procedure where the error is backpropagated instead using a fixed, random weight $b_r$.\relax }}{3}{figure.caption.1}}
\newlabel{fig:algo}{{1}{3}{Standard backpropagation updates the first layer weights for a hidden node $r$ with the second layer feedforward weight $\beta _r$. We study the procedure where the error is backpropagated instead using a fixed, random weight $b_r$.\relax }{figure.caption.1}{}}
\newlabel{fig:algo@cref}{{[figure][1][]1}{[1][3][]3}}
\newlabel{eqn:squared-loss}{{2.2}{3}{Problem Statement and Overview of Results}{equation.2.2}{}}
\newlabel{eqn:squared-loss@cref}{{[equation][2][2]2.2}{[1][3][]3}}
\citation{lillicrap2016random,bartunov,lillicrap2020backpropagation}
\citation{lillicrap2016random}
\citation{lillicrap2016random}
\newlabel{eqn:alignment-update}{{2.3}{4}{Problem Statement and Overview of Results}{equation.2.3}{}}
\newlabel{eqn:alignment-update@cref}{{[equation][3][2]2.3}{[1][4][]4}}
\citation{lillicrap2020backpropagation}
\citation{jacot2018neural}
\citation{jacot2018neural}
\citation{du2018gradient,du2019gradient,gao2020model}
\@writefile{toc}{\contentsline {section}{\numberline {3}Convergence with Random Backpropagation Weights}{5}{section.3}}
\citation{lillicrap2016random}
\newlabel{assump:G}{{3.1}{6}{}{assumption.3.1}{}}
\newlabel{assump:G@cref}{{[assumption][1][3]3.1}{[1][6][]6}}
\newlabel{thm:nonliner_conv}{{3.1}{6}{}{theorem.3.1}{}}
\newlabel{thm:nonliner_conv@cref}{{[theorem][1][3]3.1}{[1][6][]6}}
\newlabel{eq:conv}{{3.1}{6}{}{equation.3.1}{}}
\newlabel{eq:conv@cref}{{[equation][1][3]3.1}{[1][6][]6}}
\newlabel{eq:weights}{{3.2}{6}{}{equation.3.2}{}}
\newlabel{eq:weights@cref}{{[equation][2][3]3.2}{[1][6][]6}}
\newlabel{prop:positive-definiteness}{{3.1}{6}{}{proposition.3.1}{}}
\newlabel{prop:positive-definiteness@cref}{{[proposition][1][3]3.1}{[1][6][]6}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Alignment with Random Backpropagation Weights}{6}{section.4}}
\newlabel{sec:alignment}{{4}{6}{Alignment with Random Backpropagation Weights}{section.4}{}}
\newlabel{sec:alignment@cref}{{[section][4][]4}{[1][6][]6}}
\newlabel{def:alignment}{{4.1}{7}{}{definition.4.1}{}}
\newlabel{def:alignment@cref}{{[definition][1][4]4.1}{[1][6][]7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Regularized feedback alignment}{7}{subsection.4.1}}
\newlabel{eqn:loss-with-reg}{{4.1}{7}{Regularized feedback alignment}{equation.4.1}{}}
\newlabel{eqn:loss-with-reg@cref}{{[equation][1][4]4.1}{[1][7][]7}}
\newlabel{thm:nonlinear_conv_reg}{{4.1}{7}{}{theorem.4.1}{}}
\newlabel{thm:nonlinear_conv_reg@cref}{{[theorem][1][4]4.1}{[1][7][]7}}
\newlabel{eq:conv_reg}{{4.2}{7}{}{equation.4.2}{}}
\newlabel{eq:conv_reg@cref}{{[equation][2][4]4.2}{[1][7][]7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Alignment analysis for linear networks}{8}{subsection.4.2}}
\newlabel{thm:lin_conv}{{4.2}{8}{}{theorem.4.2}{}}
\newlabel{thm:lin_conv@cref}{{[theorem][2][4]4.2}{[1][8][]8}}
\newlabel{eq:reg_error_bd}{{4.3}{8}{}{equation.4.3}{}}
\newlabel{eq:reg_error_bd@cref}{{[equation][3][4]4.3}{[1][8][]8}}
\newlabel{def:isom}{{4.2}{8}{$(\gamma , \eps )$-Isometry}{definition.4.2}{}}
\newlabel{def:isom@cref}{{[definition][2][4]4.2}{[1][8][]8}}
\newlabel{prop:isom}{{4.1}{8}{}{proposition.4.1}{}}
\newlabel{prop:isom@cref}{{[proposition][1][4]4.1}{[1][8][]8}}
\newlabel{thm:lin_align}{{4.3}{8}{}{theorem.4.3}{}}
\newlabel{thm:lin_align@cref}{{[theorem][3][4]4.3}{[1][8][]8}}
\@writefile{toc}{\contentsline {paragraph}{Large sample scenario.}{9}{section*.2}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Simulations}{9}{section.5}}
\@writefile{toc}{\contentsline {paragraph}{Feedback alignment on synthetic data.}{9}{section*.3}}
\newlabel{fig:align_lr_non_autograd_l2}{{2a}{10}{Alignment on linear network.\relax }{figure.caption.4}{}}
\newlabel{fig:align_lr_non_autograd_l2@cref}{{[subfigure][1][2]2a}{[1][10][]10}}
\newlabel{sub@fig:align_lr_non_autograd_l2}{{a}{10}{Alignment on linear network.\relax }{figure.caption.4}{}}
\newlabel{sub@fig:align_lr_non_autograd_l2@cref}{{[subfigure][1][2]2a}{[1][10][]10}}
\newlabel{fig:align_nn_relu_autograd_l2}{{2b}{10}{Alignment on ReLU network.\relax }{figure.caption.4}{}}
\newlabel{fig:align_nn_relu_autograd_l2@cref}{{[subfigure][2][2]2b}{[1][10][]10}}
\newlabel{sub@fig:align_nn_relu_autograd_l2}{{b}{10}{Alignment on ReLU network.\relax }{figure.caption.4}{}}
\newlabel{sub@fig:align_nn_relu_autograd_l2@cref}{{[subfigure][2][2]2b}{[1][10][]10}}
\newlabel{fig:align_nn_tanh_autograd_l2}{{2c}{10}{Alignment on Tanh network.\relax }{figure.caption.4}{}}
\newlabel{fig:align_nn_tanh_autograd_l2@cref}{{[subfigure][3][2]2c}{[1][10][]10}}
\newlabel{sub@fig:align_nn_tanh_autograd_l2}{{c}{10}{Alignment on Tanh network.\relax }{figure.caption.4}{}}
\newlabel{sub@fig:align_nn_tanh_autograd_l2@cref}{{[subfigure][3][2]2c}{[1][10][]10}}
\newlabel{fig:loss_lr_non_autograd_l2}{{2d}{10}{Loss on linear network.\relax }{figure.caption.4}{}}
\newlabel{fig:loss_lr_non_autograd_l2@cref}{{[subfigure][4][2]2d}{[1][10][]10}}
\newlabel{sub@fig:loss_lr_non_autograd_l2}{{d}{10}{Loss on linear network.\relax }{figure.caption.4}{}}
\newlabel{sub@fig:loss_lr_non_autograd_l2@cref}{{[subfigure][4][2]2d}{[1][10][]10}}
\newlabel{fig:loss_nn_relu_autograd_l2}{{2e}{10}{Loss on ReLU network.\relax }{figure.caption.4}{}}
\newlabel{fig:loss_nn_relu_autograd_l2@cref}{{[subfigure][5][2]2e}{[1][10][]10}}
\newlabel{sub@fig:loss_nn_relu_autograd_l2}{{e}{10}{Loss on ReLU network.\relax }{figure.caption.4}{}}
\newlabel{sub@fig:loss_nn_relu_autograd_l2@cref}{{[subfigure][5][2]2e}{[1][10][]10}}
\newlabel{fig:loss_nn_tanh_autograd_l2}{{2f}{10}{Loss on Tanh network.\relax }{figure.caption.4}{}}
\newlabel{fig:loss_nn_tanh_autograd_l2@cref}{{[subfigure][6][2]2f}{[1][10][]10}}
\newlabel{sub@fig:loss_nn_tanh_autograd_l2}{{f}{10}{Loss on Tanh network.\relax }{figure.caption.4}{}}
\newlabel{sub@fig:loss_nn_tanh_autograd_l2@cref}{{[subfigure][6][2]2f}{[1][10][]10}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Comparisons of alignment and convergence for the feedback alignment algorithm with different levels of $\ell _2$ regularization. In \cref  {fig:align_lr_non_autograd_l2,fig:align_nn_relu_autograd_l2,fig:align_nn_tanh_autograd_l2}, the data points represent the mean value computed across simulations, and the error bars mark the standard deviation out of $50$ independent runs. In \cref  {fig:loss_lr_non_autograd_l2,fig:loss_nn_relu_autograd_l2,fig:loss_nn_tanh_autograd_l2}, we show the trajectories of the training loss for networks with $p = 3200$, with the shaded areas indicating the standard deviation over $50$ independent runs. The $x$-axes on the first row and the $y$-axes on the second row are presented using a logarithmic scale.\relax }}{10}{figure.caption.4}}
\newlabel{fig:synthetic-l2}{{2}{10}{Comparisons of alignment and convergence for the feedback alignment algorithm with different levels of $\ell _2$ regularization. In \cref {fig:align_lr_non_autograd_l2,fig:align_nn_relu_autograd_l2,fig:align_nn_tanh_autograd_l2}, the data points represent the mean value computed across simulations, and the error bars mark the standard deviation out of $50$ independent runs. In \cref {fig:loss_lr_non_autograd_l2,fig:loss_nn_relu_autograd_l2,fig:loss_nn_tanh_autograd_l2}, we show the trajectories of the training loss for networks with $p = 3200$, with the shaded areas indicating the standard deviation over $50$ independent runs. The $x$-axes on the first row and the $y$-axes on the second row are presented using a logarithmic scale.\relax }{figure.caption.4}{}}
\newlabel{fig:synthetic-l2@cref}{{[figure][2][]2}{[1][10][]10}}
\citation{wager2013dropout}
\citation{deng2012mnist}
\citation{lillicrap2016random}
\citation{lillicrap2016random}
\@writefile{toc}{\contentsline {paragraph}{Feedback alignment on the MNIST dataset.}{11}{section*.5}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion}{11}{section.6}}
\bibstyle{apalike}
\bibdata{feedback_alignment}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Comparisons on alignment and accuracy for feedback alignment algorithm with $\lambda =0,0.1,0.3$. The left figure shows alignment defined by $\qopname  \relax o{cos}\angle (\delta _\mathrm  {BP}(h),\delta _\mathrm  {FA}(h))$, and right figure shows the accuracy on the test set. The dashed lines and corresponding shaded areas represent the means and the standard deviations over $10$ runs with random initialization.\relax }}{12}{figure.caption.6}}
\newlabel{fig:mnist}{{3}{12}{Comparisons on alignment and accuracy for feedback alignment algorithm with $\lambda =0,0.1,0.3$. The left figure shows alignment defined by $\cos \angle (\dbp (h),\dfa (h))$, and right figure shows the accuracy on the test set. The dashed lines and corresponding shaded areas represent the means and the standard deviations over $10$ runs with random initialization.\relax }{figure.caption.6}{}}
\newlabel{fig:mnist@cref}{{[figure][3][]3}{[1][11][]12}}
\bibcite{akrout}{{1}{2019}{{Akrout et~al.}}{{}}}
\bibcite{bartunov}{{2}{2018}{{Bartunov et~al.}}{{}}}
\bibcite{bellec}{{3}{2019}{{Bellec et~al.}}{{}}}
\bibcite{deng2012mnist}{{4}{2012}{{Deng}}{{}}}
\bibcite{du2019gradient}{{5}{2019}{{Du et~al.}}{{}}}
\bibcite{du2018gradient}{{6}{2018}{{Du et~al.}}{{}}}
\bibcite{elman}{{7}{1996}{{Elman et~al.}}{{}}}
\bibcite{gao2020model}{{8}{2020}{{Gao and Lafferty}}{{}}}
\bibcite{hand2018global}{{9}{2018}{{Hand and Voroninski}}{{}}}
\bibcite{hebb1}{{10}{1961}{{Hebb}}{{}}}
\bibcite{jacot2018neural}{{11}{2018}{{Jacot et~al.}}{{}}}
\bibcite{laurent2000adaptive}{{12}{2000}{{Laurent and Massart}}{{}}}
\bibcite{lillicrap2016random}{{13}{2016}{{Lillicrap et~al.}}{{}}}
\@writefile{toc}{\contentsline {section}{Acknowledgments}{13}{section*.8}}
\@writefile{toc}{\contentsline {section}{References}{13}{section*.10}}
\bibcite{lillicrap2020backpropagation}{{14}{2020}{{Lillicrap et~al.}}{{}}}
\bibcite{medler}{{15}{1998}{{Medler}}{{}}}
\bibcite{oja}{{16}{1982}{{Oja}}{{}}}
\bibcite{paulsen}{{17}{2000}{{Paulsen and Sejnowski}}{{}}}
\bibcite{pdp}{{18}{1986a}{{Rumelhart et~al.}}{{}}}
\bibcite{rumelhart:86}{{19}{1986b}{{Rumelhart et~al.}}{{}}}
\bibcite{sejnowski2}{{20}{1999}{{Sejnowski}}{{}}}
\bibcite{sejnowski1}{{21}{1989}{{Sejnowski and Tesauro}}{{}}}
\bibcite{wager2013dropout}{{22}{2013}{{Wager et~al.}}{{}}}
\bibcite{yamins2}{{23}{2016}{{Yamins and DiCarlo}}{{}}}
\bibcite{ilker1}{{24}{2019}{{Yildirim et~al.}}{{}}}
\citation{gao2020model}
\@writefile{toc}{\contentsline {section}{\numberline {A}Convergence on Two-Layer Nonlinear Networks}{15}{appendix.A}}
\newlabel{sec:appendix-convergence}{{A}{15}{Convergence on Two-Layer Nonlinear Networks}{appendix.A}{}}
\newlabel{sec:appendix-convergence@cref}{{[appendix][1][2147483647]A}{[1][15][]15}}
\newlabel{eq:updates}{{A.3}{15}{Convergence on Two-Layer Nonlinear Networks}{equation.A.3}{}}
\newlabel{eq:updates@cref}{{[equation][3][2147483647,1]A.3}{[1][15][]15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Concentration Results}{15}{subsection.A.1}}
\newlabel{lma:G}{{A.1}{15}{Lemma A.7 in \citealp {gao2020model}}{lemma.A.1}{}}
\newlabel{lma:G@cref}{{[lemma][1][2147483647,1]A.1}{[1][15][]15}}
\newlabel{lma:inqs}{{A.2}{16}{}{lemma.A.2}{}}
\newlabel{lma:inqs@cref}{{[lemma][2][2147483647,1]A.2}{[1][15][]16}}
\newlabel{eq:sumb_bd}{{A.4}{16}{}{equation.A.4}{}}
\newlabel{eq:sumb_bd@cref}{{[equation][4][2147483647,1]A.4}{[1][15][]16}}
\newlabel{eq:sumbbet_bd}{{A.5}{16}{}{equation.A.5}{}}
\newlabel{eq:sumbbet_bd@cref}{{[equation][5][2147483647,1]A.5}{[1][15][]16}}
\newlabel{eq:e0_bd}{{A.6}{16}{}{equation.A.6}{}}
\newlabel{eq:e0_bd@cref}{{[equation][6][2147483647,1]A.6}{[1][15][]16}}
\newlabel{eq:maxb_bd}{{A.7}{16}{}{equation.A.7}{}}
\newlabel{eq:maxb_bd@cref}{{[equation][7][2147483647,1]A.7}{[1][16][]16}}
\newlabel{lma:GH}{{A.3}{16}{}{lemma.A.3}{}}
\newlabel{lma:GH@cref}{{[lemma][3][2147483647,1]A.3}{[1][16][]16}}
\newlabel{eq:def_G0}{{A.8}{16}{}{equation.A.8}{}}
\newlabel{eq:def_G0@cref}{{[equation][8][2147483647,1]A.8}{[1][16][]16}}
\newlabel{eq:def_H0}{{A.9}{16}{}{equation.A.9}{}}
\newlabel{eq:def_H0@cref}{{[equation][9][2147483647,1]A.9}{[1][16][]16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Proof of Theorem \ref  {thm:nonliner_conv}}{17}{subsection.A.2}}
\newlabel{lma:weights}{{A.4}{17}{}{lemma.A.4}{}}
\newlabel{lma:weights@cref}{{[lemma][4][2147483647,1]A.4}{[1][17][]17}}
\newlabel{lma:induction}{{A.5}{18}{}{lemma.A.5}{}}
\newlabel{lma:induction@cref}{{[lemma][5][2147483647,1]A.5}{[1][18][]18}}
\newlabel{eq:et_iter}{{A.10}{19}{Proof of Theorem \ref {thm:nonliner_conv}}{equation.A.10}{}}
\newlabel{eq:et_iter@cref}{{[equation][10][2147483647,1]A.10}{[1][18][]19}}
\newlabel{eq:bound_Gt}{{A.11}{19}{Proof of Theorem \ref {thm:nonliner_conv}}{equation.A.11}{}}
\newlabel{eq:bound_Gt@cref}{{[equation][11][2147483647,1]A.11}{[1][19][]19}}
\newlabel{eq:bound_Ht}{{A.12}{19}{Proof of Theorem \ref {thm:nonliner_conv}}{equation.A.12}{}}
\newlabel{eq:bound_Ht@cref}{{[equation][12][2147483647,1]A.12}{[1][19][]19}}
\newlabel{eq:bound_vt}{{A.13}{20}{Proof of Theorem \ref {thm:nonliner_conv}}{equation.A.13}{}}
\newlabel{eq:bound_vt@cref}{{[equation][13][2147483647,1]A.13}{[1][20][]20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Proof of Theorem \ref  {thm:nonlinear_conv_reg}}{20}{subsection.A.3}}
\newlabel{lma:weights_reg}{{A.6}{20}{}{lemma.A.6}{}}
\newlabel{lma:weights_reg@cref}{{[lemma][6][2147483647,1]A.6}{[1][20][]20}}
\newlabel{eq:weights_reg}{{A.14}{20}{}{equation.A.14}{}}
\newlabel{eq:weights_reg@cref}{{[equation][14][2147483647,1]A.14}{[1][20][]20}}
\newlabel{lma:induction_reg}{{A.7}{22}{}{lemma.A.7}{}}
\newlabel{lma:induction_reg@cref}{{[lemma][7][2147483647,1]A.7}{[1][22][]22}}
\newlabel{eq:et_iter_reg}{{A.15}{22}{Proof of Theorem \ref {thm:nonlinear_conv_reg}}{equation.A.15}{}}
\newlabel{eq:et_iter_reg@cref}{{[equation][15][2147483647,1]A.15}{[1][22][]22}}
\newlabel{eq:bound_Gt_reg}{{A.16}{22}{Proof of Theorem \ref {thm:nonlinear_conv_reg}}{equation.A.16}{}}
\newlabel{eq:bound_Gt_reg@cref}{{[equation][16][2147483647,1]A.16}{[1][22][]22}}
\newlabel{eq:bound_Ht_reg}{{A.17}{22}{Proof of Theorem \ref {thm:nonlinear_conv_reg}}{equation.A.17}{}}
\newlabel{eq:bound_Ht_reg@cref}{{[equation][17][2147483647,1]A.17}{[1][22][]22}}
\newlabel{eq:bound_vt_reg}{{A.18}{22}{Proof of Theorem \ref {thm:nonlinear_conv_reg}}{equation.A.18}{}}
\newlabel{eq:bound_vt_reg@cref}{{[equation][18][2147483647,1]A.18}{[1][22][]22}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Alignment on Two-Layer Linear Networks}{23}{appendix.B}}
\newlabel{sec:appendix-alignment}{{B}{23}{Alignment on Two-Layer Linear Networks}{appendix.B}{}}
\newlabel{sec:appendix-alignment@cref}{{[appendix][2][2147483647]B}{[1][23][]23}}
\newlabel{eq:reg_loss}{{B.1}{23}{Alignment on Two-Layer Linear Networks}{equation.B.1}{}}
\newlabel{eq:reg_loss@cref}{{[equation][1][2147483647,2]B.1}{[1][23][]23}}
\newlabel{eq:reg_alg}{{B.2}{23}{Alignment on Two-Layer Linear Networks}{equation.B.2}{}}
\newlabel{eq:reg_alg@cref}{{[equation][2][2147483647,2]B.2}{[1][23][]23}}
\newlabel{lma:one_step_update}{{B.1}{23}{}{lemma.B.1}{}}
\newlabel{lma:one_step_update@cref}{{[lemma][1][2147483647,2]B.1}{[1][23][]23}}
\newlabel{eq:err_update}{{B.3}{23}{}{equation.B.3}{}}
\newlabel{eq:err_update@cref}{{[equation][3][2147483647,2]B.3}{[1][23][]23}}
\newlabel{eq:Wt}{{B.4}{24}{Alignment on Two-Layer Linear Networks}{equation.B.4}{}}
\newlabel{eq:Wt@cref}{{[equation][4][2147483647,2]B.4}{[1][24][]24}}
\newlabel{eq:betat}{{B.5}{24}{Alignment on Two-Layer Linear Networks}{equation.B.5}{}}
\newlabel{eq:betat@cref}{{[equation][5][2147483647,2]B.5}{[1][24][]24}}
\newlabel{lma:ineqs_2}{{B.2}{25}{}{lemma.B.2}{}}
\newlabel{lma:ineqs_2@cref}{{[lemma][2][2147483647,2]B.2}{[1][25][]25}}
\newlabel{eq:bbeta_sqrtp}{{B.6}{25}{}{equation.B.6}{}}
\newlabel{eq:bbeta_sqrtp@cref}{{[equation][6][2147483647,2]B.6}{[1][25][]25}}
\newlabel{eq:bW_sqrtp}{{B.7}{25}{}{equation.B.7}{}}
\newlabel{eq:bW_sqrtp@cref}{{[equation][7][2147483647,2]B.7}{[1][25][]25}}
\newlabel{eq:b_norm}{{B.8}{25}{}{equation.B.8}{}}
\newlabel{eq:b_norm@cref}{{[equation][8][2147483647,2]B.8}{[1][25][]25}}
\newlabel{eq:ww_p}{{B.9}{25}{}{equation.B.9}{}}
\newlabel{eq:ww_p@cref}{{[equation][9][2147483647,2]B.9}{[1][25][]25}}
\newlabel{eq:J1_bd}{{B.10}{26}{Alignment on Two-Layer Linear Networks}{equation.B.10}{}}
\newlabel{eq:J1_bd@cref}{{[equation][10][2147483647,2]B.10}{[1][26][]26}}
\newlabel{eq:J2_bd}{{B.11}{26}{Alignment on Two-Layer Linear Networks}{equation.B.11}{}}
\newlabel{eq:J2_bd@cref}{{[equation][11][2147483647,2]B.11}{[1][26][]26}}
\newlabel{eq:J3_bd}{{B.12}{26}{Alignment on Two-Layer Linear Networks}{equation.B.12}{}}
\newlabel{eq:J3_bd@cref}{{[equation][12][2147483647,2]B.12}{[1][26][]26}}
\newlabel{eq:xwwx_bd}{{B.13}{26}{Alignment on Two-Layer Linear Networks}{equation.B.13}{}}
\newlabel{eq:xwwx_bd@cref}{{[equation][13][2147483647,2]B.13}{[1][26][]26}}
\newlabel{lma:suf_cond}{{B.3}{27}{}{lemma.B.3}{}}
\newlabel{lma:suf_cond@cref}{{[lemma][3][2147483647,2]B.3}{[1][27][]27}}
\newlabel{lma:decomp_1}{{B.4}{27}{}{lemma.B.4}{}}
\newlabel{lma:decomp_1@cref}{{[lemma][4][2147483647,2]B.4}{[1][27][]27}}
\newlabel{eq:at_lbd}{{B.14}{27}{}{equation.B.14}{}}
\newlabel{eq:at_lbd@cref}{{[equation][14][2147483647,2]B.14}{[1][27][]27}}
\newlabel{eq:xit_ubd}{{B.15}{27}{}{equation.B.15}{}}
\newlabel{eq:xit_ubd@cref}{{[equation][15][2147483647,2]B.15}{[1][27][]27}}
\newlabel{eq:et_y_bd}{{B.16}{28}{Alignment on Two-Layer Linear Networks}{equation.B.16}{}}
\newlabel{eq:et_y_bd@cref}{{[equation][16][2147483647,2]B.16}{[1][27][]28}}
\newlabel{lma:ST_sT}{{B.5}{28}{}{lemma.B.5}{}}
\newlabel{lma:ST_sT@cref}{{[lemma][5][2147483647,2]B.5}{[1][28][]28}}
\newlabel{eq:eXXe}{{B.17}{29}{Alignment on Two-Layer Linear Networks}{equation.B.17}{}}
\newlabel{eq:eXXe@cref}{{[equation][17][2147483647,2]B.17}{[1][28][]29}}
\newlabel{eq:exxe_e_bd}{{B.18}{30}{Alignment on Two-Layer Linear Networks}{equation.B.18}{}}
\newlabel{eq:exxe_e_bd@cref}{{[equation][18][2147483647,2]B.18}{[1][30][]30}}
\newlabel{eq:g_lbd}{{B.19}{30}{Alignment on Two-Layer Linear Networks}{equation.B.19}{}}
\newlabel{eq:g_lbd@cref}{{[equation][19][2147483647,2]B.19}{[1][30][]30}}
\newlabel{eq:g_ubd}{{B.20}{30}{Alignment on Two-Layer Linear Networks}{equation.B.20}{}}
\newlabel{eq:g_ubd@cref}{{[equation][20][2147483647,2]B.20}{[1][30][]30}}
\newlabel{eq:alphaT_eT}{{B.21}{31}{Alignment on Two-Layer Linear Networks}{equation.B.21}{}}
\newlabel{eq:alphaT_eT@cref}{{[equation][21][2147483647,2]B.21}{[1][30][]31}}
\newlabel{eq:sum_alphat}{{B.23}{31}{Alignment on Two-Layer Linear Networks}{equation.B.23}{}}
\newlabel{eq:sum_alphat@cref}{{[equation][23][2147483647,2]B.23}{[1][31][]31}}
\newlabel{eq:lhs_bound}{{B.24}{31}{Alignment on Two-Layer Linear Networks}{equation.B.24}{}}
\newlabel{eq:lhs_bound@cref}{{[equation][24][2147483647,2]B.24}{[1][31][]31}}
\citation{hand2018global}
\citation{laurent2000adaptive}
\citation{gao2020model}
\newlabel{eq:rhs_bound}{{B.25}{32}{Alignment on Two-Layer Linear Networks}{equation.B.25}{}}
\newlabel{eq:rhs_bound@cref}{{[equation][25][2147483647,2]B.25}{[1][32][]32}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Technical Lemmas}{32}{appendix.C}}
\newlabel{lem:RIP}{{C.1}{32}{\citealp {hand2018global}}{lemma.C.1}{}}
\newlabel{lem:RIP@cref}{{[lemma][1][2147483647,3]C.1}{[1][32][]32}}
\newlabel{cor:RIP}{{C.1}{32}{}{corollary.C.1}{}}
\newlabel{cor:RIP@cref}{{[corollary][1][2147483647,3]C.1}{[1][32][]32}}
\newlabel{lem:chi-squared-tail}{{C.2}{32}{\citealp {laurent2000adaptive}}{lemma.C.2}{}}
\newlabel{lem:chi-squared-tail@cref}{{[lemma][2][2147483647,3]C.2}{[1][32][]32}}
\newlabel{lem:inner-product-tail}{{C.3}{32}{\citealp {gao2020model}}{lemma.C.3}{}}
\newlabel{lem:inner-product-tail@cref}{{[lemma][3][2147483647,3]C.3}{[1][32][]32}}
