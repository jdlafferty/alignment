%!TEX root=./main.tex

\section{Convergence of Gradient Descent With Random Backpropagation Weights}

Due to the replacement of backward weights with the random backpropagation weights in feedback alignment algorithm, there is \emph{a priori} no guarantee that the algorithm will go downhill on the squared error loss $\Loss$. \citet{lillicrap2020backpropagation} study the convergence on two-layer linear networks under the continuous time setting. Through the analysis of a system of differential equations on network parameters, they show the convergence of the network to true linear target function under the assumption of infinite training data.
Among recent studies of over-parametrized networks under backpropagation, Neural Tangent Kernel (NTK) is heavily utilized to describe the evolution of the network during training process \citep{jacot2018neural}. For any neural network $f(x,\theta)$ with parameter $\theta$, the NTK is defined as
\begin{equation}
	K_f(x,y) = \Big\langle \frac{\partial f (x,\theta)}{\partial \theta},\frac{\partial f (y,\theta)}{\partial \theta}\Big\rangle.
\end{equation}
Given a dataset $\{(x_i,y_i)\}_{i=1}^n$, we can also consider its corresponding Gram matrix $K = (K_f(x_i,x_j))_{n\times n}$. \citet{jacot2018neural} show that in the infinite width limit, $K_f$ converges to a constant at initialization and does not drift away from initialization throughout training. Under the over-parameterized regime, if the Gram matrix $K$ is positive definite, then $K$ is going to stay close to the initialization during training and thus results in a linear convergence of the squared error loss \citep{du2018gradient,du2019gradient,gao2020model}.
However, after replacing the feed-forward weights $\beta$ with a random backpropagation weights $b$, feedback alignment algorithm no longer follows a strict gradient descent on the squared error loss and the previous argument cannot be applied directly to our case. In particular, for the two-layer network $f(x, \theta)$ defined in \eqref{eqn:nonlinear-network} with $\theta = (\beta,W)$, the kernel $K_f$ can be written in two parts, $G_f$ and $H_f$, which correspond to $\beta$ and $W$ respectively:
\begin{equation}
K_f(x,y) = G_f(x, y) + H_f(x,y) \defeq \Big\langle \frac{\partial f (x,\theta)}{\partial \beta},\frac{\partial f (y,\theta)}{\partial \beta}\Big\rangle + \sum_{r=1}^p\Big\langle \frac{\partial f (x,\theta)}{\partial w_r},\frac{\partial f (y,\theta)}{\partial w_r}\Big\rangle.
\end{equation}
Under feedback alignment scheme with random backward weights $b$, $G_f$ remains the same as under backpropagation while $H_f$ is no longer positive semi-definite and close to $0$ at initialization if the network is over-parameterized. However, if $G = (G_f(x_i,x_j))_{n\times n}$ is positive definite at initialization and $H = (H_f(x_i,x_j))_{n\times n}$ is small enough. Then we are able to show the loss $\Loss$ converges to zero exponentially fast. Specifically, we provide the following assumption and theorem.

\begin{assumption}\label{assump:G}
Define matrix $\bar{G} \in \R^{n\times n}$ with entries
\begin{equation*}
    \bar{G}_{i,j} = \E_{w\sim \calN(0,I_p)}\psi(w\transpose x_i) \psi(w\transpose  x_j),
\end{equation*}
we assume $\lambda_{\min}(\bar{G}) \geq \gamma$, where $\gamma$ is a positive constant.
\end{assumption}

\begin{theorem}\label{thm:nonliner_conv}
Let $W(0)$, $\beta(0)$ and $b$ have \iid standard Gaussian entries. Assume
\begin{enumerate}
    \item \cref{assump:G} holds,
    \item $\psi$ is smooth, $\psi$, $\psi'$ and $\psi''$ are bounded,
    \item $|y_i|$ and $\|x_i\|$ are bounded for all $i\in[n]$.
\end{enumerate}
Then there exists positive constants $c_1$, $c_2$, $C_1$ and $C_2$, such that for any $\delta\in(0,1)$, if 
\begin{equation*}
    p \geq \max(C_1\frac{n^2}{\delta\gamma^2}, C_2\frac{n^4\log p}{\gamma^4}),
\end{equation*}
with probability at least $1-\delta$, we have
\begin{equation}\label{eq:conv}
    \|e(t+1)\|^2 \leq (1-\frac{\eta\gamma}{4})^t\|e(t)\|^2.
\end{equation}
and 
\begin{equation}
\label{eq:weights}
    \|w_r(t)-w_r(0)\| \leq c_1\frac{n\sqrt{\log p}}{\gamma\sqrt p}, \quad |\beta_r(t)-\beta_r(0)| \leq c_2\frac{n}{\gamma\sqrt p}
\end{equation}
for all $r=1,\ldots, p$ and $t>0$.
\end{theorem}

Notably, matrix $\bar{G}$ in \cref{assump:G} is actually the expectation of $G$ over random initialization thus $G$ is close to $\bar{G}$ by concentration. To justify the assumption, we provide the following proposition, which states that \cref{assump:G} holds when $x_i$ is drawn from Gaussian distribution. The proofs of Theorem \ref{thm:nonliner_conv} and Proposition \ref{prop:positive-definiteness} are deferred to Appendix.

\begin{proposition}\label{prop:positive-definiteness}
Suppose $x_1,...,x_n \overset{\iid}{\sim} \calN(0,I_d/d)$ and the activation function $\psi$ is sigmoid or tanh. If $d=\Omega(n)$, then \cref{assump:G} holds with high probability.
\end{proposition}




