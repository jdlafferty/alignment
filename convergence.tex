\section{Convergence of Gradient Descent With Random Backpropagation Weights}

Previous studies on over-parametrized two-layer networks \citep{du2018gradient,gao2020model} focus on the Neural Tangent Kernels (NTKs) induced by the backpropagation of gradient descent that control the process of convergence. More specifically, as long as the sum of two Gram matrices $G(t)$ and $H(t)$ corresponding to the NTK of first- and second-layer weights are positive definite, the loss $L(t)$ converges at a linear rate due to the fact that $\frac{\diff L(t)}{\diff t} = -2 (y - f(t))\transpose(G(t) + K(t))(y - f(t))$. However, due to the replacement of feed-forward weights $\beta(t)$ with a random backpropagation weights $b$, feedback alignment no longer follows a strict gradient descent on the squared error loss and the previous argument cannot be applied directly to our case. Despite of this difference, we will show in this section that the linear convergence of $L(t)$ is still achievable under feedback alignment for over-parametrized two-layer networks. In particular, we have the following theorem.
\begin{theorem}[informal]
Let $W(0)$, $\beta(0)$ and $b$ have \iid standard Gaussian entries. If $\psi$ is smooth, $\psi$, $\psi'$ and $\psi''$ are bounded, $|y_i|$ and $\|x_i\|$ are bounded for all $i\in[n]$, and $\frac{d}{n}$ is large enough, then there exists universal constant $C_1$ and $C_2$, such that for any $\delta\in(0,1)$, if $p$ is large, then with probability at least $1-\delta$
\begin{equation*}
    \|u(t)-y\|^2 \leq (1-\frac{\gamma\eta}{4})^t\|u(0)-y\|^2.
\end{equation*}
\end{theorem}

